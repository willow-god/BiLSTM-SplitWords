{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e095d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "def build_train_data(file_path):   #file_path = 'data/train.txt'\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    phrase_expel = []\n",
    "    for i in lines:   #把读到的文件中的 引号 回车 和 空格 删掉\n",
    "        t1 = i.replace('“  ','')\n",
    "        t2 = t1.replace('\\n','')\n",
    "        t3 = t2.replace('  ','')\n",
    "        phrase_expel.append(t3)\n",
    "\n",
    "    with open('data/generate_pkl/train_data.pkl', 'wb') as f: #把这个处理后的文件当作训练数据\n",
    "        pkl.dump(phrase_expel, f)   #把文件写成pkl格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2fd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target(file_path):  #生成目标文件\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        tmp = f.readlines()  #tmp为每一行的数据\n",
    "\n",
    "    t = []\n",
    "    for i in tmp: #删掉引号和回车，这里不删空格，空格是分词的标志\n",
    "        t1 = i.replace('“  ','')\n",
    "        t2 = t1.replace('\\n','')\n",
    "        t.append(t2)\n",
    "\n",
    "    sum_list = []\n",
    "    for i in t:\n",
    "        sum_ = ''\n",
    "        for j in i.split():  #以空格为分割，一个词一个词的提取\n",
    "            if len(j) == 1: #如果词的长度为1 ，就标记为S -single\n",
    "                sum_ += 'S'\n",
    "                continue\n",
    "            else:\n",
    "                sum_ += 'B' #如果长度不为1，标记为一个词的开始 begin\n",
    "                for k in range(1, len(j)):\n",
    "                    if k == len(j) - 1: #如果是这个词的最后一个，就标记为end\n",
    "                        sum_ += 'E'\n",
    "                    else:\n",
    "                        sum_ += 'M'  #其他情况就是middle\n",
    "        sum_list.append(sum_)\n",
    "\n",
    "    with open('data/generate_pkl/target.pkl', 'wb') as f:\n",
    "        pkl.dump(sum_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a483b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_dict(file_path):  #'data/train_data.pkl'\n",
    "    vocab_dic = {}\n",
    "    with open(file_path, 'rb') as f:\n",
    "        z = pkl.load(f)\n",
    "        for line in z:\n",
    "            for hang in line:  #统计词频，按照词多到少排列\n",
    "                vocab_dic[hang] = vocab_dic.get(hang, 0) + 1\n",
    "        vocab_dic_sorted = sorted(vocab_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    vocab_dic2 = {word_count[0]: idx for idx, word_count in enumerate(vocab_dic_sorted)}\n",
    "    with open('data/generate_pkl/vocab.pkl', 'wb') as f:\n",
    "        pkl.dump(vocab_dic2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4321c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_train_data(\"data/sighan.txt\")# 创建原始数据文件，没有进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2858d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_target('data/sighan.txt')# 这个文件数据进行分词了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5511aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_vocab_dict('data/generate_pkl/train_data.pkl')# 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571155d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch实现双层BP神经网络实现功能\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e4c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # 参数设置类，包含一些相关参数\n",
    "    def __init__(self):\n",
    "        self.vocab = pkl.load(open('data/generate_pkl/vocab.pkl', 'rb'))  # 读取词表\n",
    "        self.train_data = pkl.load(open('data/generate_pkl/train_data.pkl', 'rb'))  # 读取训练数据\n",
    "        self.target = pkl.load(open('data/generate_pkl/target.pkl', 'rb'))  # 读取标签\n",
    "\n",
    "        self.learning_rate = 0.000015  # 学习率\n",
    "        self.epoch = 1  # epoch次数\n",
    "\n",
    "        self.output_size = 4\n",
    "        self.embed_dim = 16\n",
    "        self.hout1 = 32\n",
    "        self.hout2 = 64\n",
    "\n",
    "        self.num_layers = 2 # 测试双层LSTM神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9b0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BP神经网络\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, output_size, vocab_size, embed_dim,hout1,hout2):\n",
    "        super(Model, self).__init__()\n",
    "        #把每一个字都表示为embed_dim维的字向量\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        #隐藏层为全连接层\n",
    "        self.hid_layer1 = nn.Linear(embed_dim, hout1)\n",
    "        self.hid_layer2 = nn.Linear(hout1, hout2)\n",
    "        self.out_layer = nn.Linear(hout2, output_size)\n",
    "\n",
    "    #self指的是上面的初始化模型参数，in_layer指的是待分词的句子的张量表示\n",
    "    def forward(self, in_layer):\n",
    "        #将in_layer张量中的每一个元素（字的序号）都变成一个embed_dim维的张量\n",
    "        emd = self.embedding(in_layer)\n",
    "        #将每个字从一个embed_dim维变为hout1维的向量，神经元的出现（w，b）\n",
    "        h_out1 = self.hid_layer1(emd)\n",
    "        #将每个字从一个hout1维变为hout2维的向量，神经元的增加（w‘，b’）\n",
    "        h_out2 = self.hid_layer2(h_out1)\n",
    "        #非线性变换\n",
    "        out_ = F.relu(h_out2)\n",
    "        #将hout2维变为output_size维\n",
    "        out_ = self.out_layer(out_)\n",
    "        #每一个字都会得到到一个 为BMES的概率，最大的即为所预测的\n",
    "        out_ = F.softmax(out_, dim=1)\n",
    "        return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bed0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model_out, true_label):\n",
    "    confusion_matrix = torch.zeros([2, 2], dtype=torch.long)\n",
    "    predict_label = torch.argmax(model_out, 1)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f_1 = []\n",
    "    for l in range(4):\n",
    "        tp_num, fp_num, fn_num, tn_num = 0, 0, 0, 0\n",
    "        for p, t in zip(predict_label, true_label):\n",
    "            if p == t and t == l:\n",
    "                tp_num += 1\n",
    "            if p == l and t != l:\n",
    "                fp_num += 1\n",
    "            if p != l and p != t:\n",
    "                fn_num += 1\n",
    "            if p != l and p == t:\n",
    "                tn_num += 1\n",
    "        accuracy.append((tp_num + tn_num) / (tp_num + tn_num + fp_num + fn_num))\n",
    "        try:\n",
    "            prec = tp_num / (tp_num + fp_num)\n",
    "        except:\n",
    "            prec = 0.0\n",
    "        try:\n",
    "            rec = tp_num / (tp_num + fn_num)\n",
    "        except:\n",
    "            rec = 0\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        if prec == 0 and rec == 0:\n",
    "            f_1.append(0)\n",
    "        else:\n",
    "            f_1.append((2 * prec * rec) / (prec + rec))\n",
    "    ave_acc = torch.tensor(accuracy, dtype=torch.float).mean()\n",
    "    ave_prec = torch.tensor(precision, dtype=torch.float).mean()\n",
    "    ave_rec = torch.tensor(recall, dtype=torch.float).mean()\n",
    "    ave_f1 = torch.tensor(f_1, dtype=torch.float).mean()\n",
    "    return ave_acc, ave_prec, ave_rec, ave_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fc35f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建议注释掉这一个函数，因为test_  有可能会和内置函数重名了，改个名字也行~\n",
    "def test_Split(model_):\n",
    "    text = '在一九九八年来临之际，我十分高兴地通过中央人民广播电台、中国国际广播电台和中央电视台，向全国各族人民，向香港特别行政区同胞、澳门和台湾同胞、海外侨胞，向世界各国的朋友们，致以诚挚的问候和良好的祝愿！'\n",
    "    hang_ = []\n",
    "    for wd in text:\n",
    "        # print(wd) # test\n",
    "        hang_.append(Config().vocab[wd])\n",
    "    test_tensor = torch.tensor(hang_, dtype=torch.long)\n",
    "    res = model_(test_tensor)\n",
    "    res = res.detach().numpy()\n",
    "    [print(np.argmax(r), end=\",\") for r in res]\n",
    "    print(\"\\n\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8769b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置参数的起点\n",
    "torch.manual_seed(1)\n",
    "config = Config()\n",
    "voc_size = len(config.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d96a6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1272,  186,  669,  558,  575,  440,    1,   35,  185,  380,  194,  194,\n",
      "           5,  487,  487,  479,   14,   35,   14,  553,  294,   64,  791,  332,\n",
      "         362,   10,  247,   65]), tensor([  7, 217,   7, 342, 124, 258, 108,   3,   4,  34,  53, 426, 274, 567,\n",
      "         30])]\n"
     ]
    }
   ],
   "source": [
    "train_data_list = []\n",
    "for lin in config.train_data:\n",
    "    hang = []\n",
    "    for word in lin:\n",
    "        hang.append(config.vocab[word])\n",
    "    #将列表类型转变为张量类型\n",
    "    train_data_list.append(torch.tensor(hang, dtype=torch.long))\n",
    "print(train_data_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e3022a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = {'B': 0,\n",
    "               'M': 1,\n",
    "               'E': 2,\n",
    "               'S': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "679febcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, 1, 2, 0, 2, 0, 2, 3, 3, 0,\n",
      "        2, 3, 3, 3]), tensor([0, 1, 1, 2, 0, 1, 2, 3, 0, 2, 0, 2, 3, 0, 2])]\n"
     ]
    }
   ],
   "source": [
    "target_list = []\n",
    "for lin in config.target:\n",
    "    hang = []\n",
    "    for tag in lin:\n",
    "        hang.append(target_dict[tag])\n",
    "    target_list.append(torch.tensor(hang, dtype=torch.long))\n",
    "\n",
    "#现在 train_data_list target_list 里面的元素都是张量，它们是由张量构成的列表\n",
    "print(target_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeef90d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19056 19056\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_list), len(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8abb8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config.output_size, voc_size, config.embed_dim,config.hout1,config.hout2)\n",
    "#model __init__()函数已经生效了\n",
    "losses = []\n",
    "acc = []\n",
    "rec = []\n",
    "prec = []\n",
    "f1 = []\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)# 随机梯度下降优化器\n",
    "#model.parameters()这里的参数就是__init__()函数里面的\n",
    "loss_f = nn.CrossEntropyLoss()# 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41cfb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将代码上传到GPU进行计算\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_data_list = [i.to(device) for i in train_data_list]\n",
    "target_list = [i.to(device) for i in target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2c9dc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据进度：29.64%\tacc: 0.09090909361839294\tprec: 0.032608695328235626\trec: 0.057692307978868484\tf1: 0.04166666790843017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [17:44<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据进度：29.67%\tacc: 0.4000000059604645\tprec: 0.1666666716337204\trec: 0.125\tf1: 0.1428571492433548.06839131563901901"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Compurter_Code\\Python_language\\JupyterNotebook\\NLP_WordSplit\\BP_main.ipynb 单元格 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#模型评价\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m acc_, prec_, rec_, f1_ \u001b[39m=\u001b[39m model_eval(out, target_list[j])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m acc\u001b[39m.\u001b[39mappend(acc_\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m prec\u001b[39m.\u001b[39mappend(prec_\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;32me:\\Compurter_Code\\Python_language\\JupyterNotebook\\NLP_WordSplit\\BP_main.ipynb 单元格 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m p \u001b[39m!=\u001b[39m l \u001b[39mand\u001b[39;00m p \u001b[39m==\u001b[39m t:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         tn_num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m accuracy\u001b[39m.\u001b[39mappend((tp_num \u001b[39m+\u001b[39;49m tn_num) \u001b[39m/\u001b[39;49m (tp_num \u001b[39m+\u001b[39;49m tn_num \u001b[39m+\u001b[39;49m fp_num \u001b[39m+\u001b[39;49m fn_num))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Compurter_Code/Python_language/JupyterNotebook/NLP_WordSplit/BP_main.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     prec \u001b[39m=\u001b[39m tp_num \u001b[39m/\u001b[39m (tp_num \u001b[39m+\u001b[39m fp_num)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#实现进度条式输出\n",
    "for i in tqdm(range(config.epoch)):\n",
    "    for j, k in enumerate(train_data_list):\n",
    "        optimizer.zero_grad()\n",
    "        #开始forward的作用，预测句子k的标签\n",
    "        out = model(k)\n",
    "        #计算交叉熵损失\n",
    "        loss = loss_f(out, target_list[j])\n",
    "        #误差反向传播\n",
    "        loss.backward()\n",
    "        #参数向前更新\n",
    "        optimizer.step()\n",
    "        #模型评价\n",
    "        acc_, prec_, rec_, f1_ = model_eval(out, target_list[j])\n",
    "        acc.append(acc_.item())\n",
    "        prec.append(prec_.item())\n",
    "        rec.append(rec_.item())\n",
    "        f1.append(f1_.item())\n",
    "        print(\"\\r训练数据进度：{:.2f}%\\t\".format((j + 1) / len(train_data_list) * 100), end='')\n",
    "        print('acc: ' + str(acc_.item()) + '\\tprec: ' + str(prec_.item()) +'\\trec: ' + str(rec_.item()) + '\\tf1: ' + str(f1_.item()), end='')\n",
    "        losses.append(loss.item())\n",
    "    #保存当前的模型参数\n",
    "    torch.save(model, './model/cut_BP.bin')\n",
    "    print('\\nacc: ' + str(torch.tensor(acc).mean().item()) + '\\tprec: ' + str(torch.tensor(prec).mean().item())\n",
    "          +'\\trec: ' + str(torch.tensor(rec).mean().item()) + '\\tf1: ' + str(torch.tensor(f1).mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "241b2e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 2, 3, 3, 0, 2, 0, 2, 0, 2, 3, 0, 2, 0, 2, 3, 3, 3, 0, 2, 3,\n",
      "        3, 3, 3, 0, 2, 0, 2, 0, 2, 0, 2, 3, 0, 2, 0, 2, 3, 0, 2, 3, 0, 2, 3, 0,\n",
      "        2, 3, 0, 2, 0, 2, 0, 1, 1, 2, 0, 2, 0, 2, 0, 2, 3, 0, 2, 0, 2, 0, 2, 0,\n",
      "        2, 3, 0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 1, 1, 2, 3, 3, 0, 1, 1, 2, 3, 3,\n",
      "        0, 2, 0, 2, 3, 0, 2, 0, 2, 0, 2, 3, 0, 2, 0, 2, 3, 0, 2, 0, 1, 2, 0, 2,\n",
      "        3, 0, 2, 3, 0, 1, 2, 3, 0, 2, 0, 1, 1, 2, 3, 0, 2, 0, 1, 1, 1, 2, 0, 2,\n",
      "        0, 2, 3, 0, 2, 0, 2, 0, 2, 3, 0, 2, 3, 3, 0, 2, 3, 0, 2, 3, 0, 2, 3, 0,\n",
      "        2, 0, 2, 3, 0, 2, 0, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(target_list[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ca19b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,3,0,0,3,3,0,3,3,3,0,1,0,2,0,0,2,1,2,1,0,3,0,1,3,0,3,0,1,3,3,3,1,3,0,3,2,1,0,0,3,3,0,0,0,3,0,2,3,0,0,0,2,3,3,3,0,3,0,3,1,0,3,0,2,3,0,3,1,0,3,2,0,1,0,0,0,0,0,3,2,3,2,2,0,0,0,2,2,2,0,3,2,3,0,2,0,0,0,\n",
      "\n",
      "[[0.23297426 0.23496735 0.26104796 0.2710104 ]\n",
      " [0.25520328 0.24752448 0.23167285 0.26559937]\n",
      " [0.2721794  0.20566845 0.26142666 0.2607255 ]\n",
      " [0.2721794  0.20566845 0.26142666 0.2607255 ]\n",
      " [0.24144779 0.24482967 0.25675753 0.25696504]\n",
      " [0.2530907  0.23891412 0.2426706  0.2653246 ]\n",
      " [0.2550116  0.25013378 0.24567948 0.2491752 ]\n",
      " [0.24758793 0.22256447 0.24551104 0.28433663]\n",
      " [0.25040168 0.21062246 0.24540725 0.29356858]\n",
      " [0.25307012 0.22387812 0.25282416 0.27022767]\n",
      " [0.27516353 0.25398266 0.22589725 0.24495658]\n",
      " [0.2471822  0.26378596 0.24364814 0.24538374]\n",
      " [0.34266287 0.23132461 0.253674   0.17233846]\n",
      " [0.26627287 0.26116478 0.26640537 0.20615698]\n",
      " [0.3164399  0.24557808 0.2449774  0.1930046 ]\n",
      " [0.27278227 0.2530443  0.24716082 0.22701263]\n",
      " [0.25496632 0.21964125 0.29120457 0.23418784]\n",
      " [0.28411365 0.28611177 0.23662195 0.19315265]\n",
      " [0.2619324  0.22427973 0.263569   0.25021884]\n",
      " [0.25496423 0.26969266 0.24141347 0.23392962]\n",
      " [0.26475915 0.24286577 0.23966886 0.25270623]\n",
      " [0.24796921 0.23549157 0.25787893 0.25866026]\n",
      " [0.26797777 0.21518955 0.25425193 0.26258075]\n",
      " [0.26038384 0.27944    0.24097763 0.21919861]\n",
      " [0.24963696 0.24538645 0.25000286 0.25497368]\n",
      " [0.25799102 0.25701562 0.2560567  0.22893673]\n",
      " [0.25725436 0.23005788 0.23469189 0.2779958 ]\n",
      " [0.2814362  0.2596436  0.23867217 0.22024801]\n",
      " [0.25496423 0.26969266 0.24141347 0.23392962]\n",
      " [0.24682823 0.24527976 0.25358146 0.25431052]\n",
      " [0.24682823 0.24527976 0.25358146 0.25431052]\n",
      " [0.25307012 0.22387812 0.25282416 0.27022767]\n",
      " [0.26038384 0.27944    0.24097763 0.21919861]\n",
      " [0.24963696 0.24538645 0.25000286 0.25497368]\n",
      " [0.25799102 0.25701562 0.25605673 0.22893672]\n",
      " [0.25725436 0.23005788 0.23469189 0.2779958 ]\n",
      " [0.25504887 0.2556681  0.2557484  0.23353456]\n",
      " [0.25496423 0.26969266 0.24141347 0.23392962]\n",
      " [0.26475915 0.24286579 0.23966886 0.25270626]\n",
      " [0.25799102 0.25701562 0.25605673 0.22893672]\n",
      " [0.24991891 0.20582293 0.24158049 0.30267766]\n",
      " [0.25725436 0.23005788 0.23469189 0.2779958 ]\n",
      " [0.27516353 0.25398266 0.22589725 0.24495658]\n",
      " [0.2675604  0.2662609  0.2372106  0.22896817]\n",
      " [0.27139616 0.24062465 0.25190675 0.2360725 ]\n",
      " [0.24682823 0.24527976 0.25358146 0.25431052]\n",
      " [0.2933809  0.25050715 0.24407832 0.2120337 ]\n",
      " [0.25132224 0.26624006 0.27646118 0.20597655]\n",
      " [0.24796921 0.23549157 0.25787893 0.25866026]\n",
      " [0.26797777 0.21518955 0.25425193 0.26258075]\n",
      " [0.27516353 0.25398266 0.22589725 0.24495658]\n",
      " [0.2675604  0.2662609  0.2372106  0.22896817]\n",
      " [0.25229567 0.22724669 0.267386   0.25307158]\n",
      " [0.25386903 0.22610022 0.25507656 0.2649543 ]\n",
      " [0.25082266 0.2425488  0.24414298 0.26248553]\n",
      " [0.25477105 0.23230873 0.24322526 0.26969495]\n",
      " [0.32249323 0.2534423  0.22891288 0.19515157]\n",
      " [0.25391895 0.22468291 0.24350372 0.27789435]\n",
      " [0.27244362 0.23649094 0.23236917 0.2586962 ]\n",
      " [0.24120447 0.22987081 0.26326346 0.26566127]\n",
      " [0.26011884 0.26469633 0.24248067 0.23270424]\n",
      " [0.2814362  0.2596436  0.23867217 0.22024801]\n",
      " [0.25097728 0.23277207 0.24197504 0.27427566]\n",
      " [0.2675452  0.24782744 0.24845491 0.23617244]\n",
      " [0.25504887 0.2556681  0.2557484  0.23353456]\n",
      " [0.25725436 0.23005788 0.23469189 0.2779958 ]\n",
      " [0.25819683 0.23952907 0.2559553  0.24631873]\n",
      " [0.24120447 0.22987081 0.26326346 0.26566127]\n",
      " [0.26011884 0.26469633 0.24248067 0.23270424]\n",
      " [0.2814362  0.2596436  0.23867217 0.22024801]\n",
      " [0.24555801 0.2323782  0.2392157  0.2828481 ]\n",
      " [0.25273824 0.25461772 0.2787743  0.21386978]\n",
      " [0.2771391  0.24526145 0.2326204  0.24497908]\n",
      " [0.26011884 0.26469633 0.24248067 0.23270424]\n",
      " [0.27516353 0.25398266 0.22589725 0.24495658]\n",
      " [0.2675604  0.2662609  0.2372106  0.22896817]\n",
      " [0.32333714 0.22501986 0.24023357 0.21140942]\n",
      " [0.27037352 0.21721911 0.24331655 0.26909077]\n",
      " [0.2933809  0.25050715 0.24407832 0.2120337 ]\n",
      " [0.24682823 0.24527976 0.25358146 0.25431052]\n",
      " [0.26559678 0.24174647 0.28096396 0.21169272]\n",
      " [0.2552881  0.21265306 0.2501186  0.28194025]\n",
      " [0.23233119 0.24298064 0.2658699  0.25881824]\n",
      " [0.25192475 0.24069816 0.29916432 0.20821285]\n",
      " [0.27516353 0.25398266 0.22589725 0.24495658]\n",
      " [0.33201107 0.24357396 0.23113798 0.19327688]\n",
      " [0.27178502 0.25275147 0.25720716 0.21825635]\n",
      " [0.2631494  0.21578251 0.27131537 0.24975273]\n",
      " [0.25528347 0.23053181 0.28169224 0.23249249]\n",
      " [0.26559678 0.24174647 0.28096396 0.21169272]\n",
      " [0.25942138 0.250136   0.25631535 0.23412721]\n",
      " [0.24354647 0.22619474 0.25886357 0.27139524]\n",
      " [0.25504887 0.2556681  0.2557484  0.23353456]\n",
      " [0.2651223  0.21429674 0.23798247 0.28259853]\n",
      " [0.25998932 0.23150262 0.25309968 0.25540838]\n",
      " [0.26559678 0.24174647 0.28096396 0.21169272]\n",
      " [0.2753476  0.22341028 0.2657852  0.235457  ]\n",
      " [0.2672552  0.23492949 0.25212654 0.24568881]\n",
      " [0.275969   0.2437852  0.24625377 0.233992  ]]\n"
     ]
    }
   ],
   "source": [
    "# 将代码上传到CPU进行测试\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "train_data_list = [i.to(device) for i in train_data_list]\n",
    "target_list = [i.to(device) for i in target_list]\n",
    "#对模型的测试，可以直观地看出分词效果\n",
    "test_Split(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b75972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
