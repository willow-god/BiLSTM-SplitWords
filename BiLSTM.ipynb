{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b74b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e095d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def build_train_data(file_path):   #file_path = 'data/train.txt'\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        lines = f.read()\n",
    "    \n",
    "    # 将读取到的文本按照标点符号和换行符进行切分，得到一个列表\n",
    "    lines = re.split(r\"[，。！？、（）—《》…；“”\\n]\",lines)\n",
    "\n",
    "    # 去掉列表中的空字符串及空格\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # 去掉每个句子中的空格\n",
    "    lines = [line.replace(\" \",\"\") for line in lines]\n",
    "    \n",
    "    # 去掉所有长度大于max_len的句子\n",
    "    lines = [line for line in lines if len(line)<=max_len]\n",
    "\n",
    "    phrase_expel = lines\n",
    "\n",
    "    print(\"phrase_expel:\",phrase_expel[:10])\n",
    "    print(\"phrase_expel len:\",len(phrase_expel))\n",
    "\n",
    "    with open('data/generate_pkl/train_data.pkl', 'wb') as f: #把这个处理后的文件当作训练数据\n",
    "        pkl.dump(phrase_expel, f)   #把文件写成pkl格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4321c7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase_expel: ['迈向充满希望的新世纪', '一九九八年新年讲话', '附图片１张', '中共中央总书记', '国家主席江泽民', '一九九七年十二月三十一日', '１２月３１日', '中共中央总书记', '国家主席江泽民发表１９９８年新年讲话', '迈向充满希望的新世纪']\n",
      "phrase_expel len: 159671\n"
     ]
    }
   ],
   "source": [
    "build_train_data(\"data/sighan.txt\")# 创建原始数据文件，没有进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2fd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target(file_path):  #生成目标文件,生成字母文件\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        tmp = f.read()\n",
    "    \n",
    "    # 将读取到的文本按照标点符号和换行符进行切分，得到一个列表\n",
    "    tmp = re.split(r\"[，。！？、（）—《》…；“”\\n]\",tmp)\n",
    "\n",
    "    # 去掉列表中的空字符串\n",
    "    tmp = [line.strip() for line in tmp if line.strip()]\n",
    "\n",
    "    # 去掉每个句子首尾的空格\n",
    "    tmp = list(map(lambda x:x.strip(),tmp))\n",
    "\n",
    "    print(\"tmp:\",tmp[:10]) # 打印前10个句子\n",
    "\n",
    "    sum_list = []\n",
    "    for i in tmp:\n",
    "        sum_ = ''\n",
    "        for j in i.split():  #以空格为分割，一个词一个词的提取\n",
    "            if len(j) == 1: #如果词的长度为1 ，就标记为S -single\n",
    "                sum_ += 'S'\n",
    "                continue\n",
    "            else:\n",
    "                sum_ += 'B' #如果长度不为1，标记为一个词的开始 begin\n",
    "                for k in range(1, len(j)):\n",
    "                    if k == len(j) - 1: #如果是这个词的最后一个，就标记为end\n",
    "                        sum_ += 'E'\n",
    "                    else:\n",
    "                        sum_ += 'M'  #其他情况就是middle\n",
    "        if len(sum_) <= max_len:\n",
    "            # 如果句子长度小于等于max_len，就用\"N\"填充\n",
    "            sum_ += \"N\"*(max_len-len(sum_))\n",
    "            sum_list.append(sum_)\n",
    "\n",
    "    print(\"sum_list:\",sum_list[:10]) # 打印前10个句子\n",
    "    print(\"sum_list len:\",len(sum_list))\n",
    "\n",
    "    with open('data/generate_pkl/target.pkl', 'wb') as f:\n",
    "        pkl.dump(sum_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2858d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp: ['迈向  充满  希望  的  新  世纪', '一九九八年  新年  讲话', '附  图片  １  张', '中共中央  总书记', '国家  主席  江泽民', '一九九七年  十二月  三十一日', '１２月  ３１日', '中共中央  总书记', '国家  主席  江  泽民  发表  １９９８年  新年  讲话', '迈向  充满  希望  的  新  世纪']\n",
      "sum_list: ['BEBEBESSBENNNNNNNNNNNNNNNNNNNNNN', 'BMMMEBEBENNNNNNNNNNNNNNNNNNNNNNN', 'SBESSNNNNNNNNNNNNNNNNNNNNNNNNNNN', 'BMMEBMENNNNNNNNNNNNNNNNNNNNNNNNN', 'BEBEBMENNNNNNNNNNNNNNNNNNNNNNNNN', 'BMMMEBMEBMMENNNNNNNNNNNNNNNNNNNN', 'BMEBMENNNNNNNNNNNNNNNNNNNNNNNNNN', 'BMMEBMENNNNNNNNNNNNNNNNNNNNNNNNN', 'BEBESBEBEBMMMEBEBENNNNNNNNNNNNNN', 'BEBEBESSBENNNNNNNNNNNNNNNNNNNNNN']\n",
      "sum_list len: 159671\n"
     ]
    }
   ],
   "source": [
    "build_target('data/sighan.txt')# 这个文件数据进行分词了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a483b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_dict(file_path):  #'data/train_data.pkl'\n",
    "    vocab_dic = {}\n",
    "    with open(file_path, 'rb') as f:\n",
    "        z = pkl.load(f)\n",
    "        for line in z:\n",
    "            for hang in line:  #统计词频，按照词多到少排列\n",
    "                vocab_dic[hang] = vocab_dic.get(hang, 0) + 1\n",
    "        vocab_dic_sorted = sorted(vocab_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 按照词频排序后，构建词典，词频越高，索引越小，并且下标从1开始\n",
    "    vocab_dic2 = {}\n",
    "    for i, j in enumerate(vocab_dic_sorted):\n",
    "        vocab_dic2[j[0]] = i + 1\n",
    "\n",
    "    # 展示前10个词\n",
    "    print(\"vocab_dic2:\",list(vocab_dic2.items())[:10])\n",
    "    print(\"vocab_dic2 len:\",len(vocab_dic2))\n",
    "\n",
    "    with open('data/generate_pkl/vocab.pkl', 'wb') as f:\n",
    "        pkl.dump(vocab_dic2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5511aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_dic2: [('的', 1), ('一', 2), ('国', 3), ('在', 4), ('中', 5), ('人', 6), ('了', 7), ('是', 8), ('１', 9), ('和', 10)]\n",
      "vocab_dic2 len: 4665\n"
     ]
    }
   ],
   "source": [
    "build_vocab_dict('data/generate_pkl/train_data.pkl')# 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571155d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch实现双层BP神经网络实现功能\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00398335",
   "metadata": {},
   "source": [
    "``` python\n",
    "#BP神经网络\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size, vocab_size, embed_dim,hout1,hout2):\n",
    "        super(Model, self).__init__()\n",
    "        #把每一个字都表示为embed_dim维的字向量\n",
    "        # self.input = nn.input(vocab_size, embed_dim)  # 输入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        #隐藏层为全连接层\n",
    "        self.hid_layer1 = nn.Linear(embed_dim, hout1)\n",
    "        self.hid_layer2 = nn.Linear(hout1, hout2)\n",
    "        self.out_layer = nn.Linear(hout2, output_size)\n",
    "\n",
    "    #self指的是上面的初始化模型参数，in_layer指的是待分词的句子的张量表示\n",
    "    def forward(self, in_layer):\n",
    "        #将in_layer张量中的每一个元素（字的序号）都变成一个embed_dim维的张量\n",
    "        emd = self.embedding(in_layer)\n",
    "        #将每个字从一个embed_dim维变为hout1维的向量，神经元的出现（w，b）\n",
    "        h_out1 = self.hid_layer1(emd)\n",
    "        #将每个字从一个hout1维变为hout2维的向量，神经元的增加（w‘，b’）\n",
    "        h_out2 = self.hid_layer2(h_out1)\n",
    "        #非线性变换\n",
    "        out_ = F.relu(h_out2)\n",
    "        #将hout2维变为output_size维\n",
    "        out_ = self.out_layer(out_)\n",
    "        #每一个字都会得到到一个 为BMES的概率，最大的即为所预测的\n",
    "        out_ = F.softmax(out_, dim=1)\n",
    "        return out_\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e4c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # 参数设置类，包含一些相关参数\n",
    "    def __init__(self):\n",
    "        self.vocab = pkl.load(open('data/generate_pkl/vocab.pkl', 'rb'))  # 读取词表\n",
    "        self.train_data = pkl.load(open('data/generate_pkl/train_data.pkl', 'rb'))  # 读取训练数据\n",
    "        self.target = pkl.load(open('data/generate_pkl/target.pkl', 'rb'))  # 读取标签\n",
    "\n",
    "        self.learning_rate = 0.0015  # 学习率\n",
    "        self.epoch = 4  # epoch次数\n",
    "        self.dropout = 0.6 # dropout\n",
    "        self.max_len = 32\n",
    "\n",
    "        self.output_size = 4\n",
    "        self.embed_dim = 128\n",
    "        self.hidden_dim = 64\n",
    "        self.hout1 = 32\n",
    "        self.hout2 = 64\n",
    "\n",
    "        self.num_layers = 2 # 测试双层LSTM神经网络\n",
    "\n",
    "        print(\"---------创建参数类完成---------\")\n",
    "        print(\"# 词表大小：\", len(self.vocab))\n",
    "        print(\"# 训练数据大小：\", len(self.train_data))\n",
    "        print(\"# 标签大小：\", len(self.target))\n",
    "        print(\"# 输出大小：\", self.output_size)\n",
    "        print(\"# 嵌入层大小：\", self.embed_dim)\n",
    "        print(\"# 隐藏层大小：\", self.hidden_dim)\n",
    "        print(\"# 第一层输出大小：\", self.hout1)\n",
    "        print(\"# 第二层输出大小：\", self.hout2)\n",
    "        print(\"# 学习率：\", self.learning_rate)\n",
    "        print(\"# epoch次数：\", self.epoch)\n",
    "        print(\"# dropout层：\", self.dropout)\n",
    "        print(\"# 每个LSTM中循环次数：\", self.num_layers)\n",
    "        print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9b0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(BiLSTM_Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.bilstm1 = nn.LSTM(embedding_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.6)\n",
    "        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed0572",
   "metadata": {},
   "source": [
    "```python\n",
    "def model_eval(model_out, true_label):\n",
    "    # confusion_matrix = torch.zeros([2, 2], dtype=torch.long)\n",
    "    predict_label = torch.argmax(model_out, 1)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f_1 = []\n",
    "    for l in range(4):\n",
    "        tp_num, fp_num, fn_num, tn_num = 0, 0, 0, 0\n",
    "        for p, t in zip(predict_label, true_label):\n",
    "            if p == t and t == l:\n",
    "                tp_num += 1\n",
    "            if p == l and t != l:\n",
    "                fp_num += 1\n",
    "            if p != l and p != t:\n",
    "                fn_num += 1\n",
    "            if p != l and p == t:\n",
    "                tn_num += 1\n",
    "        accuracy.append((tp_num + tn_num) / (tp_num + tn_num + fp_num + fn_num))\n",
    "        try:\n",
    "            prec = tp_num / (tp_num + fp_num)\n",
    "        except:\n",
    "            prec = 0.0\n",
    "        try:\n",
    "            rec = tp_num / (tp_num + fn_num)\n",
    "        except:\n",
    "            rec = 0\n",
    "        precision.append(prec)\n",
    "        recall.append(rec)\n",
    "        if prec == 0 and rec == 0:\n",
    "            f_1.append(0)\n",
    "        else:\n",
    "            f_1.append((2 * prec * rec) / (prec + rec))\n",
    "    ave_acc = torch.tensor(accuracy, dtype=torch.float).mean()\n",
    "    ave_prec = torch.tensor(precision, dtype=torch.float).mean()\n",
    "    ave_rec = torch.tensor(recall, dtype=torch.float).mean()\n",
    "    ave_f1 = torch.tensor(f_1, dtype=torch.float).mean()\n",
    "    return ave_acc, ave_prec, ave_rec, ave_f1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc35f67",
   "metadata": {},
   "source": [
    "#建议注释掉这一个函数，因为test_  有可能会和内置函数重名了，改个名字也行~\n",
    "def test_Split(model_):\n",
    "    text = '在一九九八年来临之际，我十分高兴地通过中央人民广播电台、中国国际广播电台和中央电视台，向全国各族人民，向香港特别行政区同胞、澳门和台湾同胞、海外侨胞，向世界各国的朋友们，致以诚挚的问候和良好的祝愿！'\n",
    "    hang_ = []\n",
    "    for wd in text:\n",
    "        # print(wd) # test\n",
    "        hang_.append(Config().vocab[wd])\n",
    "    test_tensor = torch.tensor(hang_, dtype=torch.long)\n",
    "    res = model_(test_tensor)\n",
    "    res = res.detach().numpy()\n",
    "    [print(np.argmax(r), end=\",\") for r in res]\n",
    "    print(\"\\n\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8769b4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------创建参数类完成---------\n",
      "# 词表大小： 4665\n",
      "# 训练数据大小： 159671\n",
      "# 标签大小： 159671\n",
      "# 输出大小： 4\n",
      "# 嵌入层大小： 128\n",
      "# 隐藏层大小： 64\n",
      "# 第一层输出大小： 32\n",
      "# 第二层输出大小： 64\n",
      "# 学习率： 0.0015\n",
      "# epoch次数： 4\n",
      "# dropout层： 0.6\n",
      "# 每个LSTM中循环次数： 2\n",
      "-------------------------------\n",
      "voc_size: 4665\n"
     ]
    }
   ],
   "source": [
    "#设置参数的起点\n",
    "torch.manual_seed(1)\n",
    "config = Config()\n",
    "voc_size = len(config.vocab)\n",
    "print(\"voc_size:\", voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96a6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1259,  181,  644,  537,  557,  430,    1,   33,  183,  378,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])]\n",
      "159671\n"
     ]
    }
   ],
   "source": [
    "train_data_list = []\n",
    "for lin in config.train_data:\n",
    "    hang = []\n",
    "    for word in lin:\n",
    "        hang.append(config.vocab[word])\n",
    "    # 如果句子长度小于max_len，则在句子后面补0，使得句子长度等于max_len\n",
    "    if len(hang) < config.max_len:\n",
    "        hang.extend([0] * (config.max_len - len(hang)))\n",
    "    # 如果句子长度大于max_len，则截取句子，使得句子长度等于max_len\n",
    "    else:\n",
    "        hang = hang[:config.max_len]\n",
    "    \n",
    "    #将列表类型转变为张量类型\n",
    "    train_data_list.append(torch.tensor(hang, dtype=torch.long))\n",
    "print(train_data_list[:1])\n",
    "print(len(train_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3022a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = {'B': 0, # begin\n",
    "               'M': 1, # middle\n",
    "               'E': 2, # end\n",
    "               'S': 3, # single\n",
    "               'N': 4} # null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679febcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])]\n",
      "159671\n"
     ]
    }
   ],
   "source": [
    "# 将标签转换为one-hot编码\n",
    "target_list = []\n",
    "for lin in config.target:\n",
    "    hang = []\n",
    "    for word in lin:\n",
    "        hang.append(target_dict[word])\n",
    "    target_list.append(torch.tensor(hang, dtype=torch.long))\n",
    "print(target_list[:1])\n",
    "print(len(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeef90d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159671 159671\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_list), len(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f2380db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(train_data_list[0].shape, target_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8abb8569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_Model(\n",
      "  (embedding): Embedding(4666, 128)\n",
      "  (bilstm1): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
      "  (dropout1): Dropout(p=0.6, inplace=False)\n",
      "  (bilstm2): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
      "  (dropout2): Dropout(p=0.6, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_Model(voc_size + 1, config.embed_dim, config.hidden_dim)\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41cfb72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 将数据和模型上传到GPU进行计算\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "train_data_list = [i.to(device) for i in train_data_list]\n",
    "target_list = [i.to(device) for i in target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5db75f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159671/159671 [09:22<00:00, 283.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.033838141709566116\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159671/159671 [09:30<00:00, 279.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.056796845048666\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159671/159671 [09:45<00:00, 272.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.02790956199169159\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159671/159671 [10:43<00:00, 248.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.053907278925180435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "for epoch in range(config.epoch):\n",
    "    print(\"Epoch:\", epoch + 1)\n",
    "    for i in tqdm(range(len(train_data_list))):\n",
    "        model.zero_grad()\n",
    "        input_data = train_data_list[i].view(1, -1)\n",
    "        target = target_list[i].view(-1)\n",
    "        output = model(input_data)\n",
    "        loss = loss_function(output.view(-1, 5), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    # # 打印这个epoch的准确率\n",
    "    # with torch.no_grad():\n",
    "    #     right_num = 0\n",
    "    #     all_num = 0\n",
    "    #     for i in tqdm(range(len(train_data_list))):\n",
    "    #         input_data = train_data_list[i].view(1, -1)\n",
    "    #         target = target_list[i].view(-1)\n",
    "    #         output = model(input_data)\n",
    "    #         output = torch.argmax(output.view(-1, 5), dim=1)\n",
    "    #         for j in range(len(output)):\n",
    "    #             if output[j] == target[j]:\n",
    "    #                 right_num += 1\n",
    "    #             all_num += 1\n",
    "    #     print(\"Accuracy:\", right_num / all_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f336c96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_Model(\n",
       "  (embedding): Embedding(4666, 128)\n",
       "  (bilstm1): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
       "  (dropout1): Dropout(p=0.6, inplace=False)\n",
       "  (bilstm2): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
       "  (dropout2): Dropout(p=0.6, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model/bilstm_model.pkl')\n",
    "\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load('model/bilstm_model.pkl'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa957c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我将坚持解放思想', '抓住机遇和挑战', '中国人民将满怀信心地开创新的业绩']\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "test_data = [\"我将坚持解放思想\", \"抓住机遇和挑战\", \"中国人民将满怀信心地开创新的业绩\"]\n",
    "test_data = list(test_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e8e2699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 45, 137, 401, 200, 208, 258, 350, 306,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0]), tensor([ 590,  410,  104,  931,   10, 1145,  240,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), tensor([   5,    3,    6,   28,  137,  537, 1009,  273,  140,   21,   60,  330,\n",
      "          33,    1,   18,  869,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])]\n"
     ]
    }
   ],
   "source": [
    "# 将测试数据转换为张量\n",
    "test_data_list = []\n",
    "for lin in test_data:\n",
    "    hang = []\n",
    "    for word in lin:\n",
    "        if word in config.vocab:\n",
    "            hang.append(config.vocab[word])\n",
    "        else:\n",
    "            hang.append(0)\n",
    "    if len(hang) < config.max_len:\n",
    "        hang.extend([0] * (config.max_len - len(hang)))\n",
    "    else:\n",
    "        hang = hang[:config.max_len]\n",
    "    test_data_list.append(torch.tensor(hang, dtype=torch.long))\n",
    "print(test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a51ca986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将测试数据上传到GPU\n",
    "test_data_list = [i.to(device) for i in test_data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f67cced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-16.8229, -20.6023, -17.7427, -10.7415, -55.7657],\n",
      "         [-14.9595, -16.3279, -13.3404,  -8.5430, -49.6099],\n",
      "         [ -4.6078, -10.0039, -11.0393,  -8.2218, -64.9451],\n",
      "         [ -9.3567,  -7.0702,  -2.3992,  -6.4332, -54.9583],\n",
      "         [ -5.0555,  -9.6652, -10.4883,  -8.1819, -63.8043],\n",
      "         [ -3.1552,  -0.4953,   0.6513,  -2.0261, -36.6928],\n",
      "         [  0.8011,  -0.3517,  -2.3397,  -2.0062, -42.6504],\n",
      "         [-14.6302, -13.3675,  -5.7910,  -9.0262, -54.6771],\n",
      "         [ -3.2042,  -4.9487,  -1.8632,  -0.9067,  41.4675],\n",
      "         [ -1.5748,  -4.6515,  -1.6616,   0.3782,  53.3997],\n",
      "         [  0.3558,  -4.0154,  -0.6866,   1.8483,  58.4570],\n",
      "         [  1.5319,  -3.6111,   0.1255,   2.6160,  61.3798],\n",
      "         [  1.5740,  -3.7074,   0.3166,   2.5654,  61.5844],\n",
      "         [  1.6607,  -3.5812,   0.4877,   2.6384,  61.9165],\n",
      "         [  1.6827,  -3.5536,   0.5278,   2.6602,  62.0225],\n",
      "         [  1.6902,  -3.5449,   0.5403,   2.6679,  62.0599],\n",
      "         [  1.6937,  -3.5417,   0.5449,   2.6713,  62.0744],\n",
      "         [  1.6959,  -3.5403,   0.5468,   2.6731,  62.0811],\n",
      "         [  1.6974,  -3.5395,   0.5478,   2.6743,  62.0849],\n",
      "         [  1.6985,  -3.5390,   0.5485,   2.6752,  62.0876],\n",
      "         [  1.6995,  -3.5386,   0.5490,   2.6760,  62.0897],\n",
      "         [  1.7002,  -3.5382,   0.5495,   2.6766,  62.0917],\n",
      "         [  1.7010,  -3.5378,   0.5499,   2.6771,  62.0937],\n",
      "         [  1.7018,  -3.5373,   0.5505,   2.6776,  62.0967],\n",
      "         [  1.7032,  -3.5363,   0.5514,   2.6784,  62.1026],\n",
      "         [  1.7063,  -3.5341,   0.5535,   2.6799,  62.1163],\n",
      "         [  1.7113,  -3.5303,   0.5565,   2.6817,  62.1464],\n",
      "         [  1.7305,  -3.5156,   0.5688,   2.6911,  62.2038],\n",
      "         [  1.7392,  -3.4995,   0.5740,   2.6974,  62.2510],\n",
      "         [  1.5799,  -3.5865,   0.4256,   2.5836,  62.1297],\n",
      "         [  2.0738,  -3.1064,   0.5402,   2.7553,  61.8118],\n",
      "         [  5.7907,   0.9404,   2.3462,   4.6160,  52.2236]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "我将坚持解放思想\n",
      "tensor([[3, 3, 0, 2, 0, 2, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "SSBEBEBENNNNNNNNNNNNNNNNNNNNNNNN\n",
      "tensor([[[ -6.6943, -13.3265, -13.2363,  -8.4758, -52.3395],\n",
      "         [ -3.9397,  -3.3253,  -0.2998,  -2.1916, -35.0416],\n",
      "         [ -1.5528,  -4.8149,  -6.0321,  -4.5423, -47.5629],\n",
      "         [ -9.3990,  -8.8340,  -2.8070,  -6.2076, -46.5208],\n",
      "         [-14.5704, -15.8773, -13.2094,  -7.9914, -50.2517],\n",
      "         [ -5.7300, -12.4406, -14.6464,  -9.3490, -63.1623],\n",
      "         [-13.4568, -11.4336,  -4.2775,  -7.7818, -51.6873],\n",
      "         [ -2.9834,  -4.5860,  -1.4997,  -0.7456,  40.4271],\n",
      "         [ -2.4023,  -5.8261,  -2.9720,  -0.4460,  50.4105],\n",
      "         [ -1.4668,  -5.6525,  -2.7812,   0.3160,  53.8471],\n",
      "         [  0.3770,  -4.6571,  -1.3092,   1.6497,  58.7608],\n",
      "         [  0.8999,  -4.2925,  -0.4618,   2.0121,  59.8111],\n",
      "         [  1.5555,  -3.6392,   0.3775,   2.5616,  61.6686],\n",
      "         [  1.7005,  -3.5397,   0.5311,   2.6707,  62.0302],\n",
      "         [  1.7133,  -3.5271,   0.5576,   2.6777,  62.0757],\n",
      "         [  1.7185,  -3.5222,   0.5656,   2.6824,  62.0997],\n",
      "         [  1.7217,  -3.5198,   0.5688,   2.6851,  62.1101],\n",
      "         [  1.7240,  -3.5187,   0.5702,   2.6870,  62.1157],\n",
      "         [  1.7260,  -3.5180,   0.5711,   2.6885,  62.1197],\n",
      "         [  1.7278,  -3.5174,   0.5717,   2.6899,  62.1230],\n",
      "         [  1.7295,  -3.5169,   0.5723,   2.6911,  62.1259],\n",
      "         [  1.7310,  -3.5165,   0.5728,   2.6922,  62.1287],\n",
      "         [  1.7325,  -3.5160,   0.5734,   2.6932,  62.1315],\n",
      "         [  1.7339,  -3.5154,   0.5740,   2.6941,  62.1354],\n",
      "         [  1.7360,  -3.5143,   0.5751,   2.6952,  62.1423],\n",
      "         [  1.7396,  -3.5118,   0.5773,   2.6970,  62.1569],\n",
      "         [  1.7443,  -3.5084,   0.5800,   2.6988,  62.1860],\n",
      "         [  1.7631,  -3.4943,   0.5918,   2.7081,  62.2366],\n",
      "         [  1.7707,  -3.4787,   0.5966,   2.7146,  62.2706],\n",
      "         [  1.5867,  -3.5835,   0.4295,   2.5850,  62.1337],\n",
      "         [  2.0675,  -3.1140,   0.5351,   2.7487,  61.8307],\n",
      "         [  5.7738,   0.9219,   2.3320,   4.5992,  52.1765]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "抓住机遇和挑战\n",
      "tensor([[0, 2, 0, 2, 3, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "BEBESBENNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "tensor([[[-4.9715e+00, -1.1875e+01, -1.3615e+01, -8.3024e+00, -6.2159e+01],\n",
      "         [-9.3756e+00, -7.3494e+00, -2.5747e+00, -6.4797e+00, -5.5638e+01],\n",
      "         [-5.2408e+00, -1.0083e+01, -1.1510e+01, -8.2202e+00, -6.5527e+01],\n",
      "         [-1.1520e+01, -1.0388e+01, -3.5627e+00, -7.3682e+00, -5.0311e+01],\n",
      "         [-1.5324e+01, -1.6692e+01, -1.3938e+01, -8.6520e+00, -5.2277e+01],\n",
      "         [-5.5324e+00, -1.1241e+01, -1.2136e+01, -8.6218e+00, -6.3348e+01],\n",
      "         [-6.8881e+00, -4.1652e+00, -9.9486e-01, -4.6733e+00, -4.6985e+01],\n",
      "         [-3.0992e+00, -6.4388e+00, -7.5343e+00, -5.7620e+00, -5.2391e+01],\n",
      "         [-4.2278e+00, -3.1744e+00, -2.0038e-01, -3.0196e+00, -3.9338e+01],\n",
      "         [-3.7471e+00, -5.3977e+00, -5.1833e+00, -2.1477e+00, -2.9227e+01],\n",
      "         [-1.2461e+00, -3.2228e+00, -2.5686e+00, -1.6518e+00, -3.3463e+01],\n",
      "         [-8.2877e-01, -1.4067e+00, -1.5884e-01, -5.1321e-01, -2.6406e+01],\n",
      "         [-7.4135e+00, -8.4650e+00, -5.0757e+00, -3.6269e+00, -3.6057e+01],\n",
      "         [-1.1788e+01, -1.2768e+01, -1.0570e+01, -6.5332e+00, -4.1914e+01],\n",
      "         [-3.2384e+00, -7.7511e+00, -9.1300e+00, -6.3513e+00, -5.6372e+01],\n",
      "         [-1.2949e+01, -1.1855e+01, -4.6712e+00, -7.8065e+00, -5.2178e+01],\n",
      "         [-2.1373e+00, -3.9035e+00, -8.0325e-01, -5.6261e-02,  4.1733e+01],\n",
      "         [-2.5780e+00, -6.1243e+00, -3.3282e+00, -6.1852e-01,  4.9688e+01],\n",
      "         [-1.7130e+00, -6.1701e+00, -3.3604e+00,  7.5800e-02,  5.3013e+01],\n",
      "         [-7.3983e-02, -5.4607e+00, -2.2945e+00,  1.2068e+00,  5.7638e+01],\n",
      "         [ 8.4372e-01, -4.6417e+00, -9.2443e-01,  1.8849e+00,  5.9724e+01],\n",
      "         [ 1.1746e+00, -4.2056e+00, -1.9532e-01,  2.1814e+00,  6.0371e+01],\n",
      "         [ 1.6739e+00, -3.5748e+00,  4.9478e-01,  2.6401e+00,  6.1923e+01],\n",
      "         [ 1.7173e+00, -3.5296e+00,  5.5513e-01,  2.6787e+00,  6.2071e+01],\n",
      "         [ 1.7265e+00, -3.5203e+00,  5.6719e-01,  2.6872e+00,  6.2114e+01],\n",
      "         [ 1.7330e+00, -3.5152e+00,  5.7283e-01,  2.6917e+00,  6.2139e+01],\n",
      "         [ 1.7391e+00, -3.5108e+00,  5.7669e-01,  2.6947e+00,  6.2173e+01],\n",
      "         [ 1.7589e+00, -3.4962e+00,  5.8914e-01,  2.7049e+00,  6.2226e+01],\n",
      "         [ 1.7674e+00, -3.4803e+00,  5.9435e-01,  2.7120e+00,  6.2262e+01],\n",
      "         [ 1.5842e+00, -3.5848e+00,  4.2763e-01,  2.5830e+00,  6.2126e+01],\n",
      "         [ 2.0655e+00, -3.1151e+00,  5.3354e-01,  2.7471e+00,  6.1825e+01],\n",
      "         [ 5.7703e+00,  9.2010e-01,  2.3294e+00,  4.5964e+00,  5.2167e+01]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "中国人民将满怀信心地开创新的业绩\n",
      "tensor([[0, 2, 0, 2, 3, 0, 2, 0, 2, 3, 0, 2, 3, 3, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0')\n",
      "BEBESBEBESBESSBENNNNNNNNNNNNNNNN\n"
     ]
    }
   ],
   "source": [
    "# 开始测试\n",
    "for i in range(len(test_data_list)):\n",
    "    input_data = test_data_list[i].view(1, -1)\n",
    "    output = model(input_data)\n",
    "    print(output)\n",
    "    output = torch.argmax(output, dim=-1)\n",
    "    print(\"\".join(test_data[i]))\n",
    "    print(output)\n",
    "    print(\"\".join([['B', 'M', 'E', 'S', 'N'][i] for i in output.tolist()[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b75972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
